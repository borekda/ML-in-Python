{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Loading libraries and data\n",
    "\n",
    "This noteboook  is based on what Cristina did for preprocessing, I am adding EDA and visualisation (Daniel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "I mport the libraries"
   },
   "outputs": [],
   "source": [
    "# work with users and paths\n",
    "import getpass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import animation as ani, pyplot as plt\n",
    "import seaborn as sns #pretty graphics R style\n",
    "\n",
    "from IPython.display import HTML\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt #graphics\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import MinMaxScaler #library for the rescaling\n",
    "import statsmodels.api as sm \n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from treeinterpreter import treeinterpreter as ti, utils\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Load data"
   },
   "outputs": [],
   "source": [
    "if getpass.getuser() == 'daniel':\n",
    "    project_root_path = Path(\"/home/daniel/PhD/PhD classes/Python Machine learning/final-project\")\n",
    "    data_raw_path = project_root_path / 'data' / 'raw'\n",
    "    # Lets read the trining dataset\n",
    "    data_train = pd.read_csv(data_raw_path / 'train_V2.csv')\n",
    "    # Now we read the training data set\n",
    "    score = pd.read_csv(data_raw_path /  'score.csv')\n",
    "    # We read the dictonary\n",
    "    dict_features = pd.read_csv(data_raw_path /  'dictionary.csv', delimiter=';', header=None)\n",
    "else:\n",
    "      data_file_path = \"C:/Users/mhinojosalee/Downloads/Machine learning with Python\\Exam/\"\n",
    "      # Lets read the trining dataset\n",
    "      data_train = pd.read_csv(data_file_path+'train_V2.csv')\n",
    "      # Now we read the training data set\n",
    "      score = pd.read_csv(data_file_path+'score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Basic things to visualize the data"
   },
   "outputs": [],
   "source": [
    "print(data_train.shape)\n",
    "data_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the features of the tabular data\n",
    "Below I am importing dictionary with the description of the variables and trying to figure out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.options.display.max_columns = None\n",
    "print(data_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "I": null,
    "did": null,
    "do": null,
    "in": null,
    "incorrectly_encoded_metadata": "variable.==>",
    "not": null,
    "that": null,
    "things": null,
    "this": null,
    "title": "Visualizing the missing data, percent is the percentage of  of null data by each",
    "way": null
   },
   "outputs": [],
   "source": [
    "# total = data_train.isnull().sum().sort_values(ascending=False)\n",
    "# percent = (data_train.isnull().sum()/data_train.isnull().count()).sort_values(ascending=False)\n",
    "# (data_train.isnull().sum(axis=1))[data_train.isnull().sum(axis=1) > 30]\n",
    "# table\n",
    "# missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "# print(missing_data.head(30))\n",
    "# print(data_train['score2_pos'].value_counts())\n",
    "# data_train = data_train.drop((missing_data[missing_data['Percent'] > 0.30]).index,1)\n",
    "# data_train.dropna(inplace=True) \n",
    "#we could drop all that is NaN, but we will loose observations. (4425, 43) instead of (4425, 43) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Remove outcomes, because we will work on the missing data, and we can use the train and the test set for this (what is done in one, it has to be done in the other one), but we have to remove the outcome"
   },
   "outputs": [],
   "source": [
    "print(data_train.columns)\n",
    "data_feat = data_train.drop(['outcome_profit', 'outcome_damage_inc', 'outcome_damage_amount'], axis=1)\n",
    "print(data_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "fuse the score and the training datasets"
   },
   "outputs": [],
   "source": [
    "print(score.shape)\n",
    "datafull = pd.concat([data_feat, score])\n",
    "print(datafull.shape)\n",
    "print(datafull['client_segment'].value_counts())\n",
    "print(datafull['sect_empl'].value_counts())\n",
    "print(datafull['gender'].value_counts())\n",
    "print(datafull['retired'].value_counts())\n",
    "print(datafull['gold_status'].value_counts())\n",
    "print(datafull['prev_stay'].value_counts())\n",
    "print(datafull['divorce'].value_counts())\n",
    "print(datafull['married_cd'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Mode imputer instead of separate categories"
   },
   "outputs": [],
   "source": [
    "# datafull['client_segment'] = pd.Categorical(datafull['client_segment'])\n",
    "# datafull['sect_empl'] = pd.Categorical(datafull['sect_empl'])\n",
    "# datafull['retired'] = pd.Categorical(datafull['retired'])\n",
    "# datafull['gold_status'] = pd.Categorical(datafull['gold_status'])\n",
    "# datafull['prev_stay'] = pd.Categorical(datafull['prev_stay'])\n",
    "# datafull['divorce'] = pd.Categorical(datafull['divorce'])\n",
    "\n",
    "impute_mode = SimpleImputer (strategy='most_frequent')\n",
    "for cols in ['client_segment', \"credit_use_ic\", \"gluten_ic\", \"lactose_ic\",\"insurance_ic\",\"marketing_permit\", \"presidential\", \"urban_ic\", \"prev_all_in_stay\", \"shop_use\", \n",
    "             \"company_ic\", \"dining_ic\", \"spa_ic\",\"sport_ic\",\"empl_ic\",'sect_empl', \"retired\", \"gold_status\", \"prev_stay\", 'divorce', \"gender\"]:  \n",
    "      datafull[cols] = impute_mode.fit_transform(datafull[[cols]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Dummify them! (the ones that are not 0 and 1... Gender, sect_empl and client_segment)"
   },
   "outputs": [],
   "source": [
    "datafull['client_segment'] = pd.Categorical(datafull['client_segment'])\n",
    "datafull['sect_empl'] = pd.Categorical(datafull['sect_empl'])\n",
    "# The NaN categorie won't be necessary anymore, thanks to the mode imputing.\n",
    "pd.get_dummies(datafull[['client_segment', 'sect_empl']], dummy_na=False).head()\n",
    "print(datafull.shape)\n",
    "datafull2 = pd.concat([datafull,pd.get_dummies(datafull[['gender','client_segment', 'sect_empl']], dummy_na=False)], axis=1)\n",
    "print(datafull2.shape)\n",
    "print(datafull2.head(1000))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Dropping the original features is necessary, and I will also drop one dummy from the categorical data that I had to create \"extra columns\" for."
   },
   "outputs": [],
   "source": [
    "print(datafull2.shape)\n",
    "datafull2.drop(['client_segment', 'sect_empl', 'gender', 'client_segment_5.0','sect_empl_6.0','gender_V'], axis=1, inplace=True)\n",
    "print(datafull2.shape)\n",
    "\n",
    "datafull2['profitpernight'] = datafull2['profit_am'] / datafull2['nights_booked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Missing values per column. I decided to not drop them and use a mean imput instead."
   },
   "outputs": [],
   "source": [
    "# During class it was mentioned that sometimes it was not worthy to use a very complex thing like sof imputing to avoid dropping these features. \n",
    "# I am using something simple, that is a mean imputing, because these columns were the scores, and I did not want to drop them.\n",
    "# The scores are quantitative.\n",
    "\n",
    "impute_quant = SimpleImputer (strategy='mean')\n",
    "for cols in ['score1_pos', 'score1_neg', 'score2_pos', 'score2_neg', 'score3_pos',\n",
    "       'score3_neg', 'score4_pos', 'score4_neg', 'score5_pos', 'score5_neg']:  # Missing data, Scores are quantitative\n",
    "      datafull2[cols] = impute_quant.fit_transform(datafull2[[cols]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "No we are looking for the Missing values per row"
   },
   "outputs": [],
   "source": [
    "print(datafull2.shape)\n",
    "datafull2.dropna(thresh = datafull2.shape[1]*0.3, axis = 0, inplace = True)\n",
    "print(datafull2.shape)\n",
    "# And here we find that there are not missing values from the rows (no missing values per row). So we go to imputting the rest of the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Imputation using the mean for the other missing values"
   },
   "outputs": [],
   "source": [
    "print(datafull2.isnull().sum().sum())\n",
    "datafull2.fillna(datafull2.mean(), inplace=True)\n",
    "print(datafull2.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Time to the rescale"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "datafull3 = pd.DataFrame(scaler.fit_transform(datafull2))\n",
    "datafull3.columns = datafull2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Now the test and the train sets should be separated again (they were together just for the preprocessing so that what we do for one, we do for the other one)"
   },
   "outputs": [],
   "source": [
    "data_train = pd.concat([data_train[['outcome_profit', 'outcome_damage_inc', 'outcome_damage_amount']],datafull3[0:5000]], axis=1)\n",
    "print(data_train.shape)\n",
    "score = datafull3[5000:5500] #The score dataset will be the last 500 observations\n",
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Now it is the time to split the train and the test data sets. We decided to use 20% for testing and 80% for training"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_train.drop(['outcome_profit', 'outcome_damage_inc', 'outcome_damage_amount'],1),\n",
    "                                                    data_train['outcome_profit'], test_size=0.2, random_state=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "To answer the 2nd question, we are going to use Gradient Boost. During class it was discussed that it was a very good algorithm."
   },
   "outputs": [],
   "source": [
    "# Our model will try 500 random hyperparameter combinations, each time using 5 Cross Validation folds, totalling 2500 fits\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 50)]\n",
    "learning_rate = [x for x in np.logspace(start = -3, stop = -0.01, num = 50)]\n",
    "max_features = ['auto']\n",
    "max_depth = [int(x) for x in np.linspace(1, 10, num = 10)]\n",
    "min_samples_split = [2, 5, 10, 30]\n",
    "min_samples_leaf = [1, 2, 4, 10, 30]\n",
    "subsample = [0.4, 0.6, 0.8, 1]\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'learning_rate': learning_rate,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'subsample': subsample}\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_random = RandomizedSearchCV(estimator = gbm, param_distributions = random_grid, n_iter = 500, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "gbm_random.fit(X_train, y_train)\n",
    "gbm_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "It took around 30 to 40 minutes to run the model. So we will pickle it to not lose it."
   },
   "outputs": [],
   "source": [
    "joblib.dump(gbm_random, 'random_search_gbm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Output overview of the random search, if using spyder, this can be done in the console."
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(gbm_random.cv_results_).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "now We will inspect the best hyperparameter combination. This boasts an average test score of 0.789"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(gbm_random.cv_results_).loc[pd.DataFrame(gbm_random.cv_results_)['mean_test_score'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Now, we will fit the model for profit as asked in question 2. For this, we are using the best hyperparameters."
   },
   "outputs": [],
   "source": [
    "params = gbm_random.best_params_\n",
    "gbm_profit = GradientBoostingRegressor(**params)\n",
    "gbm_profit.fit(X_train, y_train)\n",
    "# I got R2: 0.953 for the X_train and R2: 0.822 for the X_test\n",
    "print('R2: %.3f' % gbm_profit.score(X_train, np.array(y_train).reshape(-1,1)))\n",
    "print('R2: %.3f' % gbm_profit.score(X_test, np.array(y_test).reshape(-1,1))) #Here we are using the \"holdout\" set already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "And now we are scoring the 500 potential customers with it"
   },
   "outputs": [],
   "source": [
    "profit_preds = gbm_profit.predict(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "A try to \"whiteboxing\":"
   },
   "outputs": [],
   "source": [
    "# We were interested in identifying what were the variables that mattered, so we used Variable importances based on impurity reduction.\n",
    "\n",
    "gbm_profit.feature_importances_.sum()\n",
    "d = {'feature':X_train.columns, 'importance':gbm_profit.feature_importances_}\n",
    "importances = pd.DataFrame(data=d)\n",
    "importances.sort_values('importance', ascending=False,inplace=True)\n",
    "\n",
    "plt.rcdefaults()\n",
    "plt.rcParams['figure.figsize'] = (4, 3)\n",
    "fig, ax = plt.subplots()\n",
    "variables = importances.feature\n",
    "y_pos = np.arange(len(variables))\n",
    "scaled_importance = importances.importance\n",
    "ax.barh(y_pos, scaled_importance, align='center', color='deepskyblue', ecolor='black')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(variables)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Scaled Importance')\n",
    "ax.set_title('Variable Importance')\n",
    "plt.show()\n",
    "# This graphic is unorganized. All the variables are shown in the y axis, it is difficult to read them. So Next, I will organize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Last plot is not ordered,so the next part is to order it."
   },
   "outputs": [],
   "source": [
    "importances2 = importances.copy()\n",
    "importances2 = importances2.head(20)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcdefaults()\n",
    "plt.rcParams['figure.figsize'] = (4, 3)\n",
    "fig, ax = plt.subplots()\n",
    "variables = importances2.feature\n",
    "y_pos = np.arange(len(variables))\n",
    "scaled_importance = importances2.importance\n",
    "ax.barh(y_pos, scaled_importance, align='center', color='deepskyblue', ecolor='black')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(variables)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Scaled Importance')\n",
    "ax.set_title('Variable Importance')\n",
    "plt.show()\n",
    "# And now we have a nice plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Permutation importance: As part of the whitening try:"
   },
   "outputs": [],
   "source": [
    "imp = permutation_importance(gbm_profit, X_train, y_train,n_repeats=10,\n",
    "                                random_state=42, n_jobs=2)\n",
    "\n",
    "\n",
    "\n",
    "sorted_idx = imp.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(imp.importances[sorted_idx][48:58].T,\n",
    "           vert=False, labels=X_train.columns[sorted_idx][48:58])\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "# Feature importance and Permutation importance identify the same 2 strongly predictive features for our model for Profit: Profit and Nights booked, which makes sense"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ff8aabe50d19460116dcd63cd2acfe413a2d309d9731f5dce94becb5f296dd5d"
  },
  "jupytext": {
   "cell_metadata_filter": "not,title,this,did,do,that,incorrectly_encoded_metadata,things,in,I,way,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('ml-in-python': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}