{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sun Jun 27 01:35:11 2021\n",
    "\n",
    "@author: MHinojosaLee\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "title": "I mport the libraries"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import animation as ani, pyplot as plt\n",
    "import seaborn as sns #pretty graphics R style\n",
    "\n",
    "from IPython.display import HTML\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt #graphics\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import MinMaxScaler #library for the rescaling\n",
    "import statsmodels.api as sm \n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from treeinterpreter import treeinterpreter as ti, utils\n",
    "import joblib\n",
    "import getpass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Load data"
   },
   "outputs": [],
   "source": [
    "if getpass.getuser() == 'daniel':\n",
    "    project_root_path = Path(\"/home/daniel/PhD/PhD classes/Python Machine learning/final-project\")\n",
    "    data_raw_path = project_root_path / 'data' / 'raw'\n",
    "    # Lets read the trining dataset\n",
    "    data_train = pd.read_csv(data_raw_path / 'train_V2.csv')\n",
    "    # Now we read the training data set\n",
    "    score = pd.read_csv(data_raw_path /  'score.csv')\n",
    "    # We read the dictonary\n",
    "    dict_features = pd.read_csv(data_raw_path /  'dictionary.csv', delimiter=';', header=None)\n",
    "elif getpass.getuser() == 'maart':\n",
    "    data_file_path = \"C:/Users/maart/Machine Learning/ML-in-Python/data/raw/\"\n",
    "    # Lets read the trining dataset\n",
    "    data_train = pd.read_csv(data_file_path+'train_V2.csv')\n",
    "    # Now we read the training data set\n",
    "    score = pd.read_csv(data_file_path+'score.csv')\n",
    "    dict_features_path = data_file_path+'dictionary.csv'\n",
    "    dict_features = pd.read_csv(dict_features_path, delimiter=';', header=None)\n",
    "else:\n",
    "      data_file_path = \"C:/Users/mhinojosalee/Downloads/Machine learning with Python\\Exam/\"\n",
    "      # Lets read the trining dataset\n",
    "      data_train = pd.read_csv(data_file_path+'train_V2.csv')\n",
    "      # Now we read the training data set\n",
    "      score = pd.read_csv(data_file_path+'score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Basic things to visualize the data"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 53)\n",
      "   income_am  profit_last_am  profit_am  damage_am  damage_inc  crd_lim_rec  \\\n",
      "0      227.0             0.0     3201.0      888.0         6.0      15000.0   \n",
      "1      268.0            16.0     1682.0        0.0         0.0        750.0   \n",
      "2      283.0            23.0     1673.0        0.0         0.0        750.0   \n",
      "3      227.0             0.0     1685.0        0.0         0.0          0.0   \n",
      "4     4091.0          1028.0     3425.0      785.0         2.0      14000.0   \n",
      "\n",
      "   credit_use_ic  gluten_ic  lactose_ic  insurance_ic  ...  score2_neg  \\\n",
      "0            0.0        0.0         0.0           0.0  ...         NaN   \n",
      "1            0.0        0.0         0.0           1.0  ...         NaN   \n",
      "2            0.0        0.0         0.0           1.0  ...    0.099529   \n",
      "3            0.0        0.0         0.0           0.0  ...         NaN   \n",
      "4            0.0        0.0         1.0           0.0  ...         NaN   \n",
      "\n",
      "   score3_pos  score3_neg  score4_pos  score4_neg  score5_pos  score5_neg  \\\n",
      "0         NaN         NaN    0.838147    0.082288         NaN         NaN   \n",
      "1         NaN         NaN         NaN         NaN         NaN    7.955259   \n",
      "2         NaN         NaN         NaN         NaN    0.101955    1.743020   \n",
      "3         NaN    0.889793         NaN         NaN         NaN         NaN   \n",
      "4    0.330503    0.766294    0.490486    0.542445         NaN         NaN   \n",
      "\n",
      "   outcome_profit  outcome_damage_inc  outcome_damage_amount  \n",
      "0         1791.66                   0                   0.00  \n",
      "1         1672.78                   1                 829.66  \n",
      "2         1001.40                   0                   0.00  \n",
      "3         1785.59                   0                   0.00  \n",
      "4         3140.74                   0                   0.00  \n",
      "\n",
      "[5 rows x 53 columns]\n",
      "           income_am  profit_last_am      profit_am     damage_am  \\\n",
      "count    4947.000000     4947.000000    4947.000000   4954.000000   \n",
      "mean     2281.260158      696.057712    3637.900950    145.952967   \n",
      "std      8365.254507     3051.119275    5726.625669    581.068095   \n",
      "min         0.000000        0.000000       0.000000      0.000000   \n",
      "25%       229.000000        0.000000    1638.000000      0.000000   \n",
      "50%       469.000000       52.000000    1889.000000      0.000000   \n",
      "75%      1688.000000      810.000000    3165.500000      0.000000   \n",
      "max    360577.000000   150537.000000  100577.000000  14866.000000   \n",
      "\n",
      "        damage_inc   crd_lim_rec  credit_use_ic    gluten_ic   lactose_ic  \\\n",
      "count  4947.000000   4947.000000    4947.000000  4947.000000  4947.000000   \n",
      "mean      0.352335   3298.716394       0.041237     0.024661     0.094199   \n",
      "std       0.889449   4549.646039       0.198858     0.155107     0.292134   \n",
      "min       0.000000      0.000000       0.000000     0.000000     0.000000   \n",
      "25%       0.000000      0.000000       0.000000     0.000000     0.000000   \n",
      "50%       0.000000   1500.000000       0.000000     0.000000     0.000000   \n",
      "75%       0.000000   5000.000000       0.000000     0.000000     0.000000   \n",
      "max      10.000000  30000.000000       1.000000     1.000000     1.000000   \n",
      "\n",
      "       insurance_ic       spa_ic      empl_ic  cab_requests       bar_no  \\\n",
      "count   4947.000000  4970.000000  4999.000000   4912.000000  4947.000000   \n",
      "mean       0.390944     0.401811     0.024205      6.051507     5.646250   \n",
      "std        0.488011     0.490313     0.153700      3.112104     5.052513   \n",
      "min        0.000000     0.000000     0.000000      0.000000     0.000000   \n",
      "25%        0.000000     0.000000     0.000000      3.000000     2.000000   \n",
      "50%        0.000000     0.000000     0.000000      6.000000     5.000000   \n",
      "75%        1.000000     1.000000     0.000000      9.000000     8.000000   \n",
      "max        1.000000     1.000000     1.000000     16.000000   111.000000   \n",
      "\n",
      "          sport_ic  neighbor_income          age  marketing_permit  \\\n",
      "count  4947.000000      4761.000000  4947.000000       4947.000000   \n",
      "mean      0.287043     32778.558916    44.901152          0.495452   \n",
      "std       0.452427      6858.671948    16.225094          0.500030   \n",
      "min       0.000000         0.000000    16.000000          0.000000   \n",
      "25%       0.000000     28630.000000    31.000000          0.000000   \n",
      "50%       0.000000     31990.000000    45.000000          0.000000   \n",
      "75%       1.000000     35924.000000    57.000000          1.000000   \n",
      "max       1.000000    104984.000000    97.000000          1.000000   \n",
      "\n",
      "          urban_ic    dining_ic  presidential  client_segment    sect_empl  \\\n",
      "count  4947.000000  4912.000000   4912.000000     4947.000000  4947.000000   \n",
      "mean      0.883970     0.049267      0.004275        1.298565     0.213463   \n",
      "std       0.320293     0.216447      0.065252        0.800831     0.826006   \n",
      "min       0.000000     0.000000      0.000000        0.000000     0.000000   \n",
      "25%       1.000000     0.000000      0.000000        1.000000     0.000000   \n",
      "50%       1.000000     0.000000      0.000000        1.000000     0.000000   \n",
      "75%       1.000000     0.000000      0.000000        2.000000     0.000000   \n",
      "max       1.000000     1.000000      1.000000        5.000000     6.000000   \n",
      "\n",
      "         prev_stay  prev_all_in_stay      divorce  fam_adult_size  \\\n",
      "count  4947.000000       4947.000000  4947.000000     4947.000000   \n",
      "mean      0.889832          0.252678     0.102486        1.960986   \n",
      "std       0.313130          0.434592     0.303317        0.805545   \n",
      "min       0.000000          0.000000     0.000000        1.000000   \n",
      "25%       1.000000          0.000000     0.000000        1.000000   \n",
      "50%       1.000000          0.000000     0.000000        2.000000   \n",
      "75%       1.000000          1.000000     0.000000        3.000000   \n",
      "max       1.000000          1.000000     1.000000        4.000000   \n",
      "\n",
      "       children_no   tenure_mts   tenure_yrs   company_ic    claims_no  \\\n",
      "count  4947.000000  4608.000000  4608.000000  4947.000000  4947.000000   \n",
      "mean      0.385082   273.111545    22.780165     0.018597     0.218314   \n",
      "std       0.832933   152.498416    12.719429     0.135111     0.712408   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000   154.000000    13.000000     0.000000     0.000000   \n",
      "50%       0.000000   271.000000    23.000000     0.000000     0.000000   \n",
      "75%       0.000000   368.250000    31.000000     0.000000     0.000000   \n",
      "max       6.000000   679.000000    57.000000     1.000000     9.000000   \n",
      "\n",
      "          claims_am  nights_booked       shop_am     shop_use      retired  \\\n",
      "count   4973.000000    4947.000000   4947.000000  4912.000000  4947.000000   \n",
      "mean     121.078826      28.992521    403.019960     0.151873     0.182131   \n",
      "std     1783.146726      37.480510   1335.935144     0.358934     0.385991   \n",
      "min        0.000000       1.000000      0.000000     0.000000     0.000000   \n",
      "25%        0.000000       4.000000      0.000000     0.000000     0.000000   \n",
      "50%        0.000000      11.000000      0.000000     0.000000     0.000000   \n",
      "75%        0.000000      45.000000      0.000000     0.000000     0.000000   \n",
      "max    90587.000000     375.000000  12098.364339     1.000000     1.000000   \n",
      "\n",
      "       gold_status    score1_pos    score1_neg    score2_pos    score2_neg  \\\n",
      "count  4947.000000  1.225000e+03  1.314000e+03  1.209000e+03  1.304000e+03   \n",
      "mean      0.034769  4.997356e-01  5.003663e-01  4.985522e-01  4.967340e-01   \n",
      "std       0.183212  2.879255e-01  2.887168e-01  2.877572e-01  2.897994e-01   \n",
      "min       0.000000  1.000000e-07  1.000000e-07  1.000000e-07  1.000000e-07   \n",
      "25%       0.000000  2.520205e-01  2.510338e-01  2.521282e-01  2.454209e-01   \n",
      "50%       0.000000  4.974162e-01  4.986215e-01  4.987791e-01  4.985832e-01   \n",
      "75%       0.000000  7.487276e-01  7.516726e-01  7.441403e-01  7.474935e-01   \n",
      "max       1.000000  9.999999e-01  9.999999e-01  9.999999e-01  9.986510e-01   \n",
      "\n",
      "         score3_pos    score3_neg    score4_pos    score4_neg    score5_pos  \\\n",
      "count  1.261000e+03  1.367000e+03  1.223000e+03  1.324000e+03  1.232000e+03   \n",
      "mean   4.942801e-01  4.985876e-01  4.962065e-01  5.013962e-01  5.009593e-01   \n",
      "std    2.899165e-01  2.877292e-01  2.886538e-01  2.876226e-01  2.901323e-01   \n",
      "min    1.000000e-07  1.000000e-07  1.000000e-07  1.000000e-07  1.000000e-07   \n",
      "25%    2.405574e-01  2.495061e-01  2.474100e-01  2.506703e-01  2.514905e-01   \n",
      "50%    4.942465e-01  5.016458e-01  4.933486e-01  5.020603e-01  5.029121e-01   \n",
      "75%    7.449235e-01  7.464826e-01  7.452133e-01  7.493876e-01  7.512817e-01   \n",
      "max    9.999999e-01  9.999999e-01  9.999999e-01  9.993125e-01  9.999999e-01   \n",
      "\n",
      "        score5_neg  outcome_profit  outcome_damage_inc  outcome_damage_amount  \n",
      "count  1493.000000     5000.000000         5000.000000            5000.000000  \n",
      "mean      5.192953     1967.310930            0.255400             189.970736  \n",
      "std       3.159868     1371.061266            0.436129             379.005941  \n",
      "min      -7.871775       10.680000            0.000000               0.000000  \n",
      "25%       3.124958     1333.320000            0.000000               0.000000  \n",
      "50%       5.188006     1721.235000            0.000000               0.000000  \n",
      "75%       7.357425     2223.712500            1.000000             202.612500  \n",
      "max      14.776319    31529.000000            1.000000            3157.240000  \n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape)\n",
    "print(data_train.head())\n",
    "pd.options.display.max_columns = None\n",
    "print(data_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "I": null,
    "did": null,
    "do": null,
    "in": null,
    "incorrectly_encoded_metadata": "variable.==>",
    "not": null,
    "that": null,
    "things": null,
    "this": null,
    "title": "Visualizing the missing data, percent is the percentage of  of null data by each",
    "way": null
   },
   "outputs": [],
   "source": [
    "# total = data_train.isnull().sum().sort_values(ascending=False)\n",
    "# percent = (data_train.isnull().sum()/data_train.isnull().count()).sort_values(ascending=False)\n",
    "# (data_train.isnull().sum(axis=1))[data_train.isnull().sum(axis=1) > 30]\n",
    "# table\n",
    "# missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "# print(missing_data.head(30))\n",
    "# print(data_train['score2_pos'].value_counts())\n",
    "# data_train = data_train.drop((missing_data[missing_data['Percent'] > 0.30]).index,1)\n",
    "# data_train.dropna(inplace=True) \n",
    "#we could drop all that is NaN, but we will loose observations. (4425, 43) instead of (4425, 43) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "title": "Remove outcomes, because we will work on the missing data, and we can use the train and the test set for this (what is done in one, it has to be done in the other one), but we have to remove the outcome"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['income_am', 'profit_last_am', 'profit_am', 'damage_am', 'damage_inc',\n",
      "       'crd_lim_rec', 'credit_use_ic', 'gluten_ic', 'lactose_ic',\n",
      "       'insurance_ic', 'spa_ic', 'empl_ic', 'cab_requests', 'married_cd',\n",
      "       'bar_no', 'sport_ic', 'neighbor_income', 'age', 'marketing_permit',\n",
      "       'urban_ic', 'dining_ic', 'presidential', 'client_segment', 'sect_empl',\n",
      "       'prev_stay', 'prev_all_in_stay', 'divorce', 'fam_adult_size',\n",
      "       'children_no', 'tenure_mts', 'tenure_yrs', 'company_ic', 'claims_no',\n",
      "       'claims_am', 'nights_booked', 'gender', 'shop_am', 'shop_use',\n",
      "       'retired', 'gold_status', 'score1_pos', 'score1_neg', 'score2_pos',\n",
      "       'score2_neg', 'score3_pos', 'score3_neg', 'score4_pos', 'score4_neg',\n",
      "       'score5_pos', 'score5_neg', 'outcome_profit', 'outcome_damage_inc',\n",
      "       'outcome_damage_amount'],\n",
      "      dtype='object')\n",
      "(5000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.columns)\n",
    "data_feat = data_train.drop(['outcome_profit', 'outcome_damage_inc', 'outcome_damage_amount'], axis=1)\n",
    "print(data_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "title": "fuse the score and the training datasets"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 50)\n",
      "(5500, 50)\n",
      "1.0    3712\n",
      "2.0     925\n",
      "0.0     352\n",
      "3.0     329\n",
      "4.0      87\n",
      "5.0      38\n",
      "Name: client_segment, dtype: int64\n",
      "0.0    4820\n",
      "1.0     468\n",
      "6.0      78\n",
      "2.0      45\n",
      "4.0      29\n",
      "3.0       3\n",
      "Name: sect_empl, dtype: int64\n",
      "M    2734\n",
      "V    2709\n",
      "Name: gender, dtype: int64\n",
      "0.0    4456\n",
      "1.0     987\n",
      "Name: retired, dtype: int64\n",
      "0.0    5241\n",
      "1.0     202\n",
      "Name: gold_status, dtype: int64\n",
      "1.0    4848\n",
      "0.0     595\n",
      "Name: prev_stay, dtype: int64\n",
      "0.0    4884\n",
      "1.0     559\n",
      "Name: divorce, dtype: int64\n",
      "True     4460\n",
      "False    1040\n",
      "Name: married_cd, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(score.shape)\n",
    "datafull = pd.concat([data_feat, score])\n",
    "print(datafull.shape)\n",
    "print(datafull['client_segment'].value_counts())\n",
    "print(datafull['sect_empl'].value_counts())\n",
    "print(datafull['gender'].value_counts())\n",
    "print(datafull['retired'].value_counts())\n",
    "print(datafull['gold_status'].value_counts())\n",
    "print(datafull['prev_stay'].value_counts())\n",
    "print(datafull['divorce'].value_counts())\n",
    "print(datafull['married_cd'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "title": "Mode imputer instead of separate categories"
   },
   "outputs": [],
   "source": [
    "# datafull['client_segment'] = pd.Categorical(datafull['client_segment'])\n",
    "# datafull['sect_empl'] = pd.Categorical(datafull['sect_empl'])\n",
    "# datafull['retired'] = pd.Categorical(datafull['retired'])\n",
    "# datafull['gold_status'] = pd.Categorical(datafull['gold_status'])\n",
    "# datafull['prev_stay'] = pd.Categorical(datafull['prev_stay'])\n",
    "# datafull['divorce'] = pd.Categorical(datafull['divorce'])\n",
    "\n",
    "impute_mode = SimpleImputer (strategy='most_frequent')\n",
    "for cols in ['client_segment', \"credit_use_ic\", \"gluten_ic\", \"lactose_ic\",\"insurance_ic\",\"marketing_permit\", \"presidential\", \"urban_ic\", \"prev_all_in_stay\", \"shop_use\", \n",
    "             \"company_ic\", \"dining_ic\", \"spa_ic\",\"sport_ic\",\"empl_ic\",'sect_empl', \"retired\", \"gold_status\", \"prev_stay\", 'divorce', \"gender\"]:  \n",
    "      datafull[cols] = impute_mode.fit_transform(datafull[[cols]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Dummify them! (the ones that are not 0 and 1... Gender, sect_empl and client_segment)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5500, 50)\n",
      "(5500, 64)\n",
      "     income_am  profit_last_am  profit_am  damage_am  damage_inc  crd_lim_rec  \\\n",
      "0        227.0             0.0     3201.0      888.0         6.0      15000.0   \n",
      "1        268.0            16.0     1682.0        0.0         0.0        750.0   \n",
      "2        283.0            23.0     1673.0        0.0         0.0        750.0   \n",
      "3        227.0             0.0     1685.0        0.0         0.0          0.0   \n",
      "4       4091.0          1028.0     3425.0      785.0         2.0      14000.0   \n",
      "..         ...             ...        ...        ...         ...          ...   \n",
      "995     3103.0             0.0     9466.0     1206.0         2.0      12500.0   \n",
      "996        NaN             NaN        NaN        NaN         NaN          NaN   \n",
      "997      250.0           823.0     1646.0        0.0         0.0       1500.0   \n",
      "998     6382.0           561.0     7265.0        0.0         0.0       1500.0   \n",
      "999     5556.0          2464.0     2464.0        0.0         0.0          0.0   \n",
      "\n",
      "     credit_use_ic  gluten_ic  lactose_ic  insurance_ic  spa_ic  empl_ic  \\\n",
      "0              0.0        0.0         0.0           0.0     1.0      0.0   \n",
      "1              0.0        0.0         0.0           1.0     1.0      0.0   \n",
      "2              0.0        0.0         0.0           1.0     0.0      0.0   \n",
      "3              0.0        0.0         0.0           0.0     0.0      0.0   \n",
      "4              0.0        0.0         1.0           0.0     1.0      0.0   \n",
      "..             ...        ...         ...           ...     ...      ...   \n",
      "995            1.0        1.0         1.0           1.0     1.0      0.0   \n",
      "996            0.0        0.0         0.0           0.0     0.0      0.0   \n",
      "997            0.0        0.0         0.0           1.0     1.0      0.0   \n",
      "998            0.0        0.0         0.0           0.0     0.0      0.0   \n",
      "999            0.0        0.0         0.0           0.0     0.0      0.0   \n",
      "\n",
      "     cab_requests  married_cd  bar_no  sport_ic  neighbor_income   age  \\\n",
      "0             3.0        True     2.0       1.0          28936.0  37.0   \n",
      "1             7.0        True     3.0       0.0          16674.0  18.0   \n",
      "2             1.0        True     4.0       0.0          32552.0  21.0   \n",
      "3             6.0        True     8.0       1.0          32252.0  37.0   \n",
      "4             4.0       False     2.0       1.0          29605.0  26.0   \n",
      "..            ...         ...     ...       ...              ...   ...   \n",
      "995           5.0        True    12.0       1.0          32920.0  46.0   \n",
      "996           NaN       False     NaN       0.0              NaN   NaN   \n",
      "997           7.0        True     5.0       0.0          29285.0  37.0   \n",
      "998           1.0        True     2.0       0.0          42836.0  56.0   \n",
      "999           4.0        True     4.0       0.0          35505.0  64.0   \n",
      "\n",
      "     marketing_permit  urban_ic  dining_ic  presidential client_segment  \\\n",
      "0                 0.0       1.0        0.0           0.0            1.0   \n",
      "1                 0.0       0.0        0.0           0.0            1.0   \n",
      "2                 0.0       1.0        0.0           0.0            1.0   \n",
      "3                 0.0       1.0        0.0           0.0            1.0   \n",
      "4                 0.0       1.0        0.0           0.0            2.0   \n",
      "..                ...       ...        ...           ...            ...   \n",
      "995               0.0       1.0        0.0           0.0            2.0   \n",
      "996               0.0       1.0        0.0           0.0            1.0   \n",
      "997               1.0       1.0        0.0           0.0            1.0   \n",
      "998               0.0       1.0        0.0           0.0            2.0   \n",
      "999               0.0       1.0        0.0           0.0            2.0   \n",
      "\n",
      "    sect_empl  prev_stay  prev_all_in_stay  divorce  fam_adult_size  \\\n",
      "0         1.0        1.0               1.0      0.0             3.0   \n",
      "1         0.0        0.0               0.0      0.0             1.0   \n",
      "2         0.0        1.0               0.0      0.0             1.0   \n",
      "3         0.0        1.0               0.0      0.0             3.0   \n",
      "4         0.0        1.0               0.0      0.0             2.0   \n",
      "..        ...        ...               ...      ...             ...   \n",
      "995       0.0        1.0               1.0      0.0             3.0   \n",
      "996       0.0        1.0               0.0      0.0             NaN   \n",
      "997       0.0        1.0               0.0      0.0             3.0   \n",
      "998       0.0        0.0               0.0      0.0             3.0   \n",
      "999       0.0        1.0               0.0      0.0             1.0   \n",
      "\n",
      "     children_no  tenure_mts  tenure_yrs  company_ic  claims_no  claims_am  \\\n",
      "0            2.0       476.0        40.0         0.0        0.0        0.0   \n",
      "1            0.0        27.0         2.0         0.0        0.0        0.0   \n",
      "2            0.0        95.0         8.0         0.0        0.0        0.0   \n",
      "3            2.0         NaN         NaN         0.0        0.0        0.0   \n",
      "4            0.0       354.0        30.0         0.0        0.0        0.0   \n",
      "..           ...         ...         ...         ...        ...        ...   \n",
      "995          2.0       314.0        26.0         0.0        1.0        0.0   \n",
      "996          NaN         NaN         NaN         0.0        NaN        NaN   \n",
      "997          0.0        26.0         2.0         0.0        0.0        0.0   \n",
      "998          0.0       361.0        30.0         0.0        1.0        0.0   \n",
      "999          0.0       209.0        17.0         0.0        0.0        0.0   \n",
      "\n",
      "     nights_booked gender      shop_am  shop_use  retired  gold_status  \\\n",
      "0            209.0      M     0.000000       0.0      0.0          0.0   \n",
      "1              4.0      M     0.000000       0.0      0.0          0.0   \n",
      "2              6.0      M     0.000000       0.0      0.0          0.0   \n",
      "3              4.0      V     0.000000       0.0      0.0          0.0   \n",
      "4              3.0      V  1454.210627       1.0      0.0          0.0   \n",
      "..             ...    ...          ...       ...      ...          ...   \n",
      "995          158.0      M   475.946777       1.0      0.0          1.0   \n",
      "996            NaN      M          NaN       0.0      0.0          0.0   \n",
      "997           11.0      V     0.000000       0.0      0.0          0.0   \n",
      "998           22.0      V     0.000000       0.0      0.0          0.0   \n",
      "999            4.0      V     0.000000       0.0      1.0          0.0   \n",
      "\n",
      "     score1_pos  score1_neg  score2_pos  score2_neg  score3_pos  score3_neg  \\\n",
      "0      0.467768    0.983340         NaN         NaN         NaN         NaN   \n",
      "1           NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "2           NaN         NaN    0.232375    0.099529         NaN         NaN   \n",
      "3           NaN         NaN         NaN         NaN         NaN    0.889793   \n",
      "4           NaN         NaN         NaN         NaN    0.330503    0.766294   \n",
      "..          ...         ...         ...         ...         ...         ...   \n",
      "995    0.581640    0.293551         NaN         NaN         NaN         NaN   \n",
      "996         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "997    0.074012    0.742951    0.914448    0.459659         NaN         NaN   \n",
      "998         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "999    0.450102    0.724313    0.481430    0.201374         NaN         NaN   \n",
      "\n",
      "     score4_pos  score4_neg  score5_pos  score5_neg  gender_M  gender_V  \\\n",
      "0      0.838147    0.082288         NaN         NaN         1         0   \n",
      "1           NaN         NaN         NaN    7.955259         1         0   \n",
      "2           NaN         NaN    0.101955    1.743020         1         0   \n",
      "3           NaN         NaN         NaN         NaN         0         1   \n",
      "4      0.490486    0.542445         NaN         NaN         0         1   \n",
      "..          ...         ...         ...         ...       ...       ...   \n",
      "995         NaN         NaN         NaN         NaN         1         0   \n",
      "996         NaN         NaN         NaN         NaN         1         0   \n",
      "997         NaN         NaN         NaN         NaN         0         1   \n",
      "998         NaN         NaN         NaN         NaN         0         1   \n",
      "999         NaN         NaN    0.648959    1.811479         0         1   \n",
      "\n",
      "     client_segment_0.0  client_segment_1.0  client_segment_2.0  \\\n",
      "0                     0                   1                   0   \n",
      "1                     0                   1                   0   \n",
      "2                     0                   1                   0   \n",
      "3                     0                   1                   0   \n",
      "4                     0                   0                   1   \n",
      "..                  ...                 ...                 ...   \n",
      "995                   0                   0                   1   \n",
      "996                   0                   1                   0   \n",
      "997                   0                   1                   0   \n",
      "998                   0                   0                   1   \n",
      "999                   0                   0                   1   \n",
      "\n",
      "     client_segment_3.0  client_segment_4.0  client_segment_5.0  \\\n",
      "0                     0                   0                   0   \n",
      "1                     0                   0                   0   \n",
      "2                     0                   0                   0   \n",
      "3                     0                   0                   0   \n",
      "4                     0                   0                   0   \n",
      "..                  ...                 ...                 ...   \n",
      "995                   0                   0                   0   \n",
      "996                   0                   0                   0   \n",
      "997                   0                   0                   0   \n",
      "998                   0                   0                   0   \n",
      "999                   0                   0                   0   \n",
      "\n",
      "     sect_empl_0.0  sect_empl_1.0  sect_empl_2.0  sect_empl_3.0  \\\n",
      "0                0              1              0              0   \n",
      "1                1              0              0              0   \n",
      "2                1              0              0              0   \n",
      "3                1              0              0              0   \n",
      "4                1              0              0              0   \n",
      "..             ...            ...            ...            ...   \n",
      "995              1              0              0              0   \n",
      "996              1              0              0              0   \n",
      "997              1              0              0              0   \n",
      "998              1              0              0              0   \n",
      "999              1              0              0              0   \n",
      "\n",
      "     sect_empl_4.0  sect_empl_6.0  \n",
      "0                0              0  \n",
      "1                0              0  \n",
      "2                0              0  \n",
      "3                0              0  \n",
      "4                0              0  \n",
      "..             ...            ...  \n",
      "995              0              0  \n",
      "996              0              0  \n",
      "997              0              0  \n",
      "998              0              0  \n",
      "999              0              0  \n",
      "\n",
      "[1000 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "datafull['client_segment'] = pd.Categorical(datafull['client_segment'])\n",
    "datafull['sect_empl'] = pd.Categorical(datafull['sect_empl'])\n",
    "# The NaN categorie won't be necessary anymore, thanks to the mode imputing.\n",
    "pd.get_dummies(datafull[['client_segment', 'sect_empl']], dummy_na=False).head()\n",
    "print(datafull.shape)\n",
    "datafull2 = pd.concat([datafull,pd.get_dummies(datafull[['gender','client_segment', 'sect_empl']], dummy_na=False)], axis=1)\n",
    "print(datafull2.shape)\n",
    "print(datafull2.head(1000))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "title": "Dropping the original features is necessary, and I will also drop one dummy from the categorical data that I had to create \"extra columns\" for."
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5500, 64)\n",
      "(5500, 58)\n"
     ]
    }
   ],
   "source": [
    "print(datafull2.shape)\n",
    "datafull2.drop(['client_segment', 'sect_empl', 'gender', 'client_segment_5.0','sect_empl_6.0','gender_V'], axis=1, inplace=True)\n",
    "print(datafull2.shape)\n",
    "\n",
    "datafull2['profitpernight'] = datafull2['profit_am'] / datafull2['nights_booked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "title": "Missing values per column. I decided to not drop them and use a mean imput instead."
   },
   "outputs": [],
   "source": [
    "# During class it was mentioned that sometimes it was not worthy to use a very complex thing like sof imputing to avoid dropping these features. \n",
    "# I am using something simple, that is a mean imputing, because these columns were the scores, and I did not want to drop them.\n",
    "# The scores are quantitative.\n",
    "\n",
    "impute_quant = SimpleImputer (strategy='mean')\n",
    "for cols in ['score1_pos', 'score1_neg', 'score2_pos', 'score2_neg', 'score3_pos',\n",
    "       'score3_neg', 'score4_pos', 'score4_neg', 'score5_pos', 'score5_neg']:  # Missing data, Scores are quantitative\n",
    "      datafull2[cols] = impute_quant.fit_transform(datafull2[[cols]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "title": "No we are looking for the Missing values per row"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5500, 59)\n",
      "(5500, 59)\n"
     ]
    }
   ],
   "source": [
    "print(datafull2.shape)\n",
    "datafull2.dropna(thresh = datafull2.shape[1]*0.3, axis = 0, inplace = True)\n",
    "print(datafull2.shape)\n",
    "# And here we find that there are not missing values from the rows (no missing values per row). So we go to imputting the rest of the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "title": "Imputation using the mean for the other missing values"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2033\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(datafull2.isnull().sum().sum())\n",
    "datafull2.fillna(datafull2.mean(), inplace=True)\n",
    "print(datafull2.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "title": "Time to the rescale"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "datafull3 = pd.DataFrame(scaler.fit_transform(datafull2))\n",
    "datafull3.columns = datafull2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "title": "Now the test and the train sets should be separated again (they were together just for the preprocessing so that what we do for one, we do for the other one)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 62)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(500, 59)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.concat([data_train[['outcome_profit', 'outcome_damage_inc', 'outcome_damage_amount']],datafull3[0:5000]], axis=1)\n",
    "print(data_train.shape)\n",
    "score = datafull3[5000:5500] #The score dataset will be the last 500 observations\n",
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "title": "Now it is the time to split the train and the test data sets. We decided to use 20% for testing and 80% for training"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_train.drop(['outcome_profit', 'outcome_damage_inc', 'outcome_damage_amount'],1),\n",
    "                                                    data_train['outcome_profit'], test_size=0.2, random_state=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "To answer the 2nd question, we are going to use Gradient Boost. During class it was discussed that it was a very good algorithm."
   },
   "outputs": [],
   "source": [
    "# Our model will try 500 random hyperparameter combinations, each time using 5 Cross Validation folds, totalling 2500 fits\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 50)]\n",
    "learning_rate = [x for x in np.logspace(start = -3, stop = -0.01, num = 50)]\n",
    "max_features = ['auto']\n",
    "max_depth = [int(x) for x in np.linspace(1, 10, num = 10)]\n",
    "min_samples_split = [2, 5, 10, 30]\n",
    "min_samples_leaf = [1, 2, 4, 10, 30]\n",
    "subsample = [0.4, 0.6, 0.8, 1]\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'learning_rate': learning_rate,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'subsample': subsample}\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_random = RandomizedSearchCV(estimator = gbm, param_distributions = random_grid, n_iter = 500, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "gbm_random.fit(X_train, y_train)\n",
    "gbm_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "It took around 30 to 40 minutes to run the model. So we will pickle it to not lose it."
   },
   "outputs": [],
   "source": [
    "joblib.dump(gbm_random, 'random_search_gbm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Output overview of the random search, if using spyder, this can be done in the console."
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(gbm_random.cv_results_).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "now We will inspect the best hyperparameter combination. This boasts an average test score of 0.789"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(gbm_random.cv_results_).loc[pd.DataFrame(gbm_random.cv_results_)['mean_test_score'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Now, we will fit the model for profit as asked in question 2. For this, we are using the best hyperparameters."
   },
   "outputs": [],
   "source": [
    "params = gbm_random.best_params_\n",
    "gbm_profit = GradientBoostingRegressor(**params)\n",
    "gbm_profit.fit(X_train, y_train)\n",
    "# I got R2: 0.953 for the X_train and R2: 0.822 for the X_test\n",
    "print('R2: %.3f' % gbm_profit.score(X_train, np.array(y_train).reshape(-1,1)))\n",
    "print('R2: %.3f' % gbm_profit.score(X_test, np.array(y_test).reshape(-1,1))) #Here we are using the \"holdout\" set already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "And now we are scoring the 500 potential customers with it"
   },
   "outputs": [],
   "source": [
    "profit_preds = gbm_profit.predict(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "A try to \"whiteboxing\":"
   },
   "outputs": [],
   "source": [
    "# We were interested in identifying what were the variables that mattered, so we used Variable importances based on impurity reduction.\n",
    "\n",
    "gbm_profit.feature_importances_.sum()\n",
    "d = {'feature':X_train.columns, 'importance':gbm_profit.feature_importances_}\n",
    "importances = pd.DataFrame(data=d)\n",
    "importances.sort_values('importance', ascending=False,inplace=True)\n",
    "\n",
    "plt.rcdefaults()\n",
    "plt.rcParams['figure.figsize'] = (4, 3)\n",
    "fig, ax = plt.subplots()\n",
    "variables = importances.feature\n",
    "y_pos = np.arange(len(variables))\n",
    "scaled_importance = importances.importance\n",
    "ax.barh(y_pos, scaled_importance, align='center', color='deepskyblue', ecolor='black')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(variables)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Scaled Importance')\n",
    "ax.set_title('Variable Importance')\n",
    "plt.show()\n",
    "# This graphic is unorganized. All the variables are shown in the y axis, it is difficult to read them. So Next, I will organize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Last plot is not ordered,so the next part is to order it."
   },
   "outputs": [],
   "source": [
    "importances2 = importances.copy()\n",
    "importances2 = importances2.head(20)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcdefaults()\n",
    "plt.rcParams['figure.figsize'] = (4, 3)\n",
    "fig, ax = plt.subplots()\n",
    "variables = importances2.feature\n",
    "y_pos = np.arange(len(variables))\n",
    "scaled_importance = importances2.importance\n",
    "ax.barh(y_pos, scaled_importance, align='center', color='deepskyblue', ecolor='black')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(variables)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Scaled Importance')\n",
    "ax.set_title('Variable Importance')\n",
    "plt.show()\n",
    "# And now we have a nice plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Permutation importance: As part of the whitening try:"
   },
   "outputs": [],
   "source": [
    "imp = permutation_importance(gbm_profit, X_train, y_train,n_repeats=10,\n",
    "                                random_state=42, n_jobs=2)\n",
    "\n",
    "\n",
    "\n",
    "sorted_idx = imp.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(imp.importances[sorted_idx][48:58].T,\n",
    "           vert=False, labels=X_train.columns[sorted_idx][48:58])\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "# Feature importance and Permutation importance identify the same 2 strongly predictive features for our model for Profit: Profit and Nights booked, which makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of damage (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3723\n",
       "1    1277\n",
       "Name: outcome_damage_inc, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.outcome_damage_inc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 25.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'subsample': 0.8,\n",
       " 'n_estimators': 761,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 30,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 7,\n",
       " 'learning_rate': 0.004690557776399856}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_train.drop(['outcome_profit', 'outcome_damage_inc', 'outcome_damage_amount'],1), \n",
    "                                                    data_train['outcome_damage_inc'], test_size=0.2, random_state=9876)\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 50)]\n",
    "learning_rate = [x for x in np.logspace(start = -3, stop = -0.01, num = 50)]\n",
    "max_features = ['auto']\n",
    "max_depth = [int(x) for x in np.linspace(1, 10, num = 10)]\n",
    "min_samples_split = [2, 5, 10, 30]\n",
    "min_samples_leaf = [1, 2, 4, 10, 30]\n",
    "subsample = [0.4, 0.6, 0.8, 1]\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'learning_rate': learning_rate,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'subsample': subsample}\n",
    "gbm = GradientBoostingClassifier()\n",
    "gbm_random = RandomizedSearchCV(estimator = gbm, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=4872, n_jobs = -1)\n",
    "gbm_random.fit(X_train, y_train)\n",
    "gbm_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.829\n",
      "Test accuracy: 0.730\n"
     ]
    }
   ],
   "source": [
    "params = gbm_random.best_params_\n",
    "gbm_damagebin = GradientBoostingClassifier(**params)\n",
    "gbm_damagebin.fit(X_train, y_train)\n",
    "print('Train accuracy: %.3f' % gbm_damagebin.score(X_train, y_train))\n",
    "print('Test accuracy: %.3f' % gbm_damagebin.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "damagebin_preds = gbm_damagebin.predict_proba(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "not,title,this,did,do,that,incorrectly_encoded_metadata,things,in,I,way,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
